{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the validation 0/1 loss is:  0.055\n"
     ]
    }
   ],
   "source": [
    "class cifar10vgg:\n",
    "    def __init__(self):\n",
    "        self.num_classes = 10\n",
    "        self.weight_decay = 0.0005\n",
    "        self.x_shape = [32,32,3]\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.model.load_weights('cifar10vgg.h5')\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.\n",
    "\n",
    "        model = Sequential()\n",
    "        weight_decay = self.weight_decay\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                         input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.num_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def normalize_production(self,x):\n",
    "        mean = 120.707\n",
    "        std = 64.15\n",
    "        return (x-mean)/(std+1e-7)\n",
    "\n",
    "    def predict(self,x,normalize=True,batch_size=50):\n",
    "        if normalize:\n",
    "            x = self.normalize_production(x)\n",
    "        return self.model.predict(x,batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    model = cifar10vgg()\n",
    "\n",
    "    predicted_x = model.predict(x_test[0:200,:])\n",
    "    residuals = np.argmax(predicted_x,1)!=np.argmax(y_test[0:200],1)\n",
    "\n",
    "    loss = sum(residuals)/len(residuals)\n",
    "    print(\"the validation 0/1 loss is: \",loss)\n",
    "    \n",
    "    cnn = model.model\n",
    "w1  = cnn.get_layer('conv2d_1').get_weights()\n",
    "w2  = cnn.get_layer('conv2d_2').get_weights()\n",
    "w3  = cnn.get_layer('conv2d_3').get_weights()\n",
    "w4  = cnn.get_layer('conv2d_4').get_weights()\n",
    "w5  = cnn.get_layer('conv2d_5').get_weights()\n",
    "w6  = cnn.get_layer('conv2d_6').get_weights()\n",
    "w7  = cnn.get_layer('conv2d_7').get_weights()\n",
    "w8  = cnn.get_layer('conv2d_8').get_weights()\n",
    "w9  = cnn.get_layer('conv2d_9').get_weights()\n",
    "w10 = cnn.get_layer('conv2d_10').get_weights()\n",
    "w11 = cnn.get_layer('conv2d_11').get_weights()\n",
    "w12 = cnn.get_layer('conv2d_12').get_weights()\n",
    "w13 = cnn.get_layer('conv2d_13').get_weights()\n",
    "\n",
    "\n",
    "bn1 = cnn.get_layer('batch_normalization_1').get_weights()\n",
    "bn2 = cnn.get_layer('batch_normalization_2').get_weights()\n",
    "bn3 = cnn.get_layer('batch_normalization_3').get_weights()\n",
    "bn4 = cnn.get_layer('batch_normalization_4').get_weights()\n",
    "bn5 = cnn.get_layer('batch_normalization_5').get_weights()\n",
    "bn6 = cnn.get_layer('batch_normalization_6').get_weights()\n",
    "bn7 = cnn.get_layer('batch_normalization_7').get_weights()\n",
    "bn8 = cnn.get_layer('batch_normalization_8').get_weights()\n",
    "bn9 = cnn.get_layer('batch_normalization_9').get_weights()\n",
    "bn10 = cnn.get_layer('batch_normalization_10').get_weights()\n",
    "bn11 = cnn.get_layer('batch_normalization_11').get_weights()\n",
    "bn12 = cnn.get_layer('batch_normalization_12').get_weights()\n",
    "bn13 = cnn.get_layer('batch_normalization_13').get_weights()\n",
    "bn14 = cnn.get_layer('batch_normalization_14').get_weights()\n",
    "\n",
    "w14 = cnn.get_layer('dense_1').get_weights()\n",
    "w15 = cnn.get_layer('dense_2').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "def map2(x,scalar=10,predictor_bits=5):\n",
    "    \n",
    "    sign = tf.math.sign(x)\n",
    "    sign = tf.dtypes.cast(sign, tf.int32)\n",
    "    \n",
    "    y = tf.math.abs(x)\n",
    "    y = tf.math.scalar_mul(2<<scalar, y)\n",
    "    y = tf.dtypes.cast(y, tf.int32)\n",
    "    y = tf.bitwise.right_shift(y, (scalar - predictor_bits + 1))\n",
    "    \n",
    "    result =  tf.math.multiply(sign, y)\n",
    "    result = tf.dtypes.cast(result, tf.float16)\n",
    "    return result\n",
    "def map_bias(x,scalar=10,predictor_bits=5):\n",
    "    \n",
    "    sign = tf.math.sign(x)\n",
    "    sign = tf.dtypes.cast(sign, tf.int32)\n",
    "    y = tf.math.abs(x)\n",
    "    y = tf.math.scalar_mul(2<<scalar, y)\n",
    "    y = tf.dtypes.cast(y, tf.int32)\n",
    "    y = tf.bitwise.right_shift(y, (scalar - predictor_bits + 1))\n",
    "    result =  tf.math.multiply(sign, y)\n",
    "    result = tf.math.scalar_mul(2<<predictor_bits, result)\n",
    "    result = tf.dtypes.cast(result, tf.float16)\n",
    "    return result\n",
    "\n",
    "def relu_predictor(x,w,b, conv, predictor_bits = 5):\n",
    "    mapped_i = map2(x, 10, predictor_bits)\n",
    "    mapped_w = map2(w, 13, predictor_bits+3)\n",
    "    mapped_b = map_bias(b, 10, predictor_bits)\n",
    "    predictions = tf.nn.conv2d(mapped_i, mapped_w, strides=[1,1,1,1], padding='SAME')\n",
    "    predictions = tf.nn.bias_add(predictions, mapped_b)\n",
    "    predictions = tf.math.greater_equal(predictions, 0)\n",
    "    zeros = tf.zeros_like(conv)\n",
    "    predictions = tf.where(predictions, conv, zeros)\n",
    "    return predictions\n",
    "   \n",
    "def tf_model(x):\n",
    "\n",
    "    conv1 = tf.nn.conv2d(x, w1[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.bias_add(conv1, w1[1])\n",
    "    rel1 = tf.nn.relu(conv1)\n",
    "    batch_norm1 =tf.layers.batch_normalization(rel1, beta_initializer=tf.constant_initializer(bn1[1]),gamma_initializer=tf.constant_initializer(bn1[0]),moving_mean_initializer=tf.constant_initializer(bn1[2]),moving_variance_initializer=tf.constant_initializer(bn1[3]))\n",
    "    conv2 = tf.nn.conv2d(batch_norm1, w2[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.bias_add(conv2, w2[1])\n",
    "    rel2 = tf.nn.relu(conv2)\n",
    "    batch_norm2 =tf.layers.batch_normalization(rel2,beta_initializer=tf.constant_initializer(bn2[1]),gamma_initializer=tf.constant_initializer(bn2[0]),moving_mean_initializer=tf.constant_initializer(bn2[2]),moving_variance_initializer=tf.constant_initializer(bn2[3]))\n",
    "    pool1 = tf.nn.max_pool(batch_norm2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID');\n",
    "    \n",
    "    conv3 = tf.nn.conv2d(pool1, w3[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv3 = tf.nn.bias_add(conv3, w3[1])\n",
    "    rel3 = tf.nn.relu(conv3)\n",
    "    \n",
    "              \n",
    "    rel_out1 = rel3\n",
    "    rel_pred_out1 = relu_predictor(pool1, w3[0], w3[1], conv3, 4)\n",
    "    \n",
    "    batch_norm3 =tf.layers.batch_normalization(rel_pred_out1, beta_initializer=tf.constant_initializer(bn3[1]),gamma_initializer=tf.constant_initializer(bn3[0]),moving_mean_initializer=tf.constant_initializer(bn3[2]),moving_variance_initializer=tf.constant_initializer(bn3[3]))\n",
    "    conv4 = tf.nn.conv2d(batch_norm3, w4[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv4 = tf.nn.bias_add(conv4, w4[1])\n",
    "    rel4 = tf.nn.relu(conv4)\n",
    "    batch_norm4 =tf.layers.batch_normalization(rel4,beta_initializer=tf.constant_initializer(bn4[1]),gamma_initializer=tf.constant_initializer(bn4[0]),moving_mean_initializer=tf.constant_initializer(bn4[2]),moving_variance_initializer=tf.constant_initializer(bn4[3]))\n",
    "    pool2 = tf.nn.max_pool(batch_norm4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID');\n",
    "\n",
    "    \n",
    "    conv5 = tf.nn.conv2d(pool2, w5[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv5 = tf.nn.bias_add(conv5, w5[1])\n",
    "    rel5 = tf.nn.relu(conv5)\n",
    "    batch_norm5 =tf.layers.batch_normalization(rel5,beta_initializer=tf.constant_initializer(bn5[1]),gamma_initializer=tf.constant_initializer(bn5[0]),moving_mean_initializer=tf.constant_initializer(bn5[2]),moving_variance_initializer=tf.constant_initializer(bn5[3]))\n",
    "    conv6 = tf.nn.conv2d(batch_norm5, w6[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv6 = tf.nn.bias_add(conv6, w6[1])\n",
    "    rel6 = tf.nn.relu(conv6)\n",
    "    \n",
    "    rel_out2 = rel6\n",
    "    rel_pred_out2 = relu_predictor(batch_norm5, w6[0], w6[1], conv6, 3)\n",
    "    \n",
    "    batch_norm6 =tf.layers.batch_normalization(rel_pred_out2,beta_initializer=tf.constant_initializer(bn6[1]),gamma_initializer=tf.constant_initializer(bn6[0]),moving_mean_initializer=tf.constant_initializer(bn6[2]),moving_variance_initializer=tf.constant_initializer(bn6[3]))\n",
    "    conv7 = tf.nn.conv2d(batch_norm6, w7[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv7 = tf.nn.bias_add(conv7, w7[1])\n",
    "    rel7 = tf.nn.relu(conv7)\n",
    "    batch_norm7 =tf.layers.batch_normalization(rel7,beta_initializer=tf.constant_initializer(bn7[1]),gamma_initializer=tf.constant_initializer(bn7[0]),moving_mean_initializer=tf.constant_initializer(bn7[2]),moving_variance_initializer=tf.constant_initializer(bn7[3]))\n",
    "    pool3 = tf.nn.max_pool(batch_norm7, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID');\n",
    "    \n",
    "    \n",
    "    conv8 = tf.nn.conv2d(pool3, w8[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv8 = tf.nn.bias_add(conv8, w8[1])\n",
    "    rel8 = tf.nn.relu(conv8)    \n",
    "    batch_norm8 = tf.layers.batch_normalization(rel8,beta_initializer=tf.constant_initializer(bn8[1]),gamma_initializer=tf.constant_initializer(bn8[0]),moving_mean_initializer=tf.constant_initializer(bn8[2]),moving_variance_initializer=tf.constant_initializer(bn8[3]))\n",
    "    conv9 = tf.nn.conv2d(batch_norm8, w9[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv9 = tf.nn.bias_add(conv9, w9[1])\n",
    "    rel9 = tf.nn.relu(conv9)    \n",
    "    \n",
    "        \n",
    "    rel_out3 = rel9\n",
    "    rel_pred_out3 = relu_predictor(batch_norm8, w9[0], w9[1], conv9, 2)\n",
    "    \n",
    "    batch_norm9 = tf.layers.batch_normalization(rel_pred_out3,beta_initializer=tf.constant_initializer(bn9[1]),gamma_initializer=tf.constant_initializer(bn9[0]),moving_mean_initializer=tf.constant_initializer(bn9[2]),moving_variance_initializer=tf.constant_initializer(bn9[3]))\n",
    "    conv10 = tf.nn.conv2d(batch_norm9, w10[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv10 = tf.nn.bias_add(conv10, w10[1])\n",
    "    rel10 = tf.nn.relu(conv10)\n",
    "    batch_norm10 =tf.layers.batch_normalization(rel10,beta_initializer=tf.constant_initializer(bn10[1]),gamma_initializer=tf.constant_initializer(bn10[0]),moving_mean_initializer=tf.constant_initializer(bn10[2]),moving_variance_initializer=tf.constant_initializer(bn10[3]))\n",
    "    pool4 = tf.nn.max_pool(batch_norm10, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID');\n",
    "    conv11 = tf.nn.conv2d(pool4, w11[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv11 = tf.nn.bias_add(conv11, w11[1])\n",
    "    rel11 = tf.nn.relu(conv11)  \n",
    "    batch_norm11 =tf.layers.batch_normalization(rel11,beta_initializer=tf.constant_initializer(bn11[1]),gamma_initializer=tf.constant_initializer(bn11[0]),moving_mean_initializer=tf.constant_initializer(bn11[2]),moving_variance_initializer=tf.constant_initializer(bn11[3]))\n",
    "    conv12 = tf.nn.conv2d(batch_norm11, w12[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv12 = tf.nn.bias_add(conv12, w12[1])\n",
    "    rel12 = tf.nn.relu(conv12)\n",
    "    \n",
    "    rel_out4 = rel12\n",
    "    rel_pred_out4 = relu_predictor(batch_norm11, w12[0], w12[1], conv12, 2)\n",
    "    \n",
    "    batch_norm12 =tf.layers.batch_normalization(rel_pred_out4,beta_initializer=tf.constant_initializer(bn12[1]),gamma_initializer=tf.constant_initializer(bn12[0]),moving_mean_initializer=tf.constant_initializer(bn12[2]),moving_variance_initializer=tf.constant_initializer(bn12[3]))\n",
    "    conv13 = tf.nn.conv2d(batch_norm12, w13[0], strides=[1,1,1,1], padding='SAME')\n",
    "    conv13 = tf.nn.bias_add(conv13, w13[1])\n",
    "    rel13 = tf.nn.relu(conv13)\n",
    "    batch_norm13 = tf.layers.batch_normalization(rel13,beta_initializer=tf.constant_initializer(bn13[1]),gamma_initializer=tf.constant_initializer(bn13[0]),moving_mean_initializer=tf.constant_initializer(bn13[2]),moving_variance_initializer=tf.constant_initializer(bn13[3]))\n",
    "    pool5 = tf.nn.max_pool(batch_norm13, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID');\n",
    "    \n",
    "    \n",
    "    flat = tf.layers.flatten(pool5)  \n",
    "    dense1 = tf.layers.dense(flat, 512, activation=tf.nn.relu, use_bias=True, kernel_initializer=tf.constant_initializer(w14[0]),bias_initializer=tf.constant_initializer(w14[1]))\n",
    "    #batch_norm14 =bn14[0]* (dense1 - bn14[2]) / np.sqrt(bn14[3] + 1e-3) + bn14[1]\n",
    "    batch_norm14 = tf.layers.batch_normalization(dense1,beta_initializer=tf.constant_initializer(bn14[1]),gamma_initializer=tf.constant_initializer(bn14[0]),moving_mean_initializer=tf.constant_initializer(bn14[2]),moving_variance_initializer=tf.constant_initializer(bn14[3]))\n",
    "    dense2 = tf.layers.dense(batch_norm14, 10, activation=None, use_bias=True, kernel_initializer=tf.constant_initializer(w15[0]),bias_initializer=tf.constant_initializer(w15[1]))\n",
    "    desne2 = tf.keras.activations.softmax(dense2)\n",
    "    return (dense2, rel_out1, rel_out2, rel_out3, rel_out4, rel_pred_out1, rel_pred_out2, rel_pred_out3, rel_pred_out4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the validation 0/1 loss is:  0.0716\n",
      "predictor accuracy of layer 3 is  0.9524689422607422\n",
      "predictor accuracy of layer 6 is  0.8456027221679687\n",
      "predictor accuracy of layer 9 is  0.4143244384765625\n",
      "predictor accuracy of layer 12 is  0.42389794921875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean = 120.707\n",
    "std = 64.15\n",
    "inp = (x_test[0:10000,:]-mean)/(std+1e-7)\n",
    "\n",
    "\n",
    "logits = tf_model(inp)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    (output, rel1, rel2, rel3, rel4, rel_pred1, rel_pred2, rel_pred3, rel_pred4) = sess.run(logits)\n",
    "    predicted_labels = np.argmax(output,1)  \n",
    "    actual_labels = np.argmax(y_test[0:10000],1)\n",
    "    residuals = predicted_labels != actual_labels\n",
    "    loss = sum(residuals)/len(residuals)\n",
    "    print(\"the validation 0/1 loss is: \",loss)\n",
    "    \n",
    "    tmp1 =np.equal(rel1,rel_pred1)\n",
    "    tmp_loss1 = np.sum(tmp1)/tmp1.size\n",
    "    print(\"predictor accuracy of layer 3 is \",tmp_loss1)\n",
    "    \n",
    "    tmp2 =np.equal(rel2,rel_pred2)\n",
    "    tmp_loss2 = np.sum(tmp2)/tmp2.size\n",
    "    print(\"predictor accuracy of layer 6 is \",tmp_loss2)\n",
    "    \n",
    "    tmp3 =np.equal(rel3,rel_pred3)\n",
    "    tmp_loss3 = np.sum(tmp3)/tmp3.size\n",
    "    print(\"predictor accuracy of layer 9 is \",tmp_loss3)\n",
    "    \n",
    "    tmp4 =np.equal(rel4,rel_pred4)\n",
    "    tmp_loss4 = np.sum(tmp4)/tmp4.size\n",
    "    print(\"predictor accuracy of layer 12 is \",tmp_loss4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
