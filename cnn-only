{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cifar10vgg:\n",
    "    def __init__(self,train=False):\n",
    "        self.num_classes = 10\n",
    "        self.weight_decay = 0.0005\n",
    "        self.x_shape = [32,32,3]\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        if train:\n",
    "            self.model = self.train(self.model)\n",
    "        else:\n",
    "            self.model.load_weights('cifar10vgg.h5')\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.\n",
    "\n",
    "        model = Sequential()\n",
    "        weight_decay = self.weight_decay\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                         input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.num_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def normalize(self,X_train,X_test):\n",
    "        #this function normalize inputs for zero mean and unit variance\n",
    "        # it is used when training a model.\n",
    "        # Input: training set and test set\n",
    "        # Output: normalized training set and test set according to the trianing set statistics.\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7)\n",
    "        return X_train, X_test\n",
    "\n",
    "    def normalize_production(self,x):\n",
    "        #this function is used to normalize instances in production according to saved training set statistics\n",
    "        # Input: X - a training set\n",
    "        # Output X - a normalized training set according to normalization constants.\n",
    "\n",
    "        #these values produced during first training and are general for the standard cifar10 training set normalization\n",
    "        mean = 120.707\n",
    "        std = 64.15\n",
    "        return (x-mean)/(std+1e-7)\n",
    "\n",
    "    def predict(self,x,normalize=True,batch_size=50):\n",
    "        if normalize:\n",
    "            x = self.normalize_production(x)\n",
    "        return self.model.predict(x,batch_size)\n",
    "\n",
    "    def train(self,model):\n",
    "\n",
    "        #training parameters\n",
    "        batch_size = 128\n",
    "        maxepoches = 250\n",
    "        learning_rate = 0.1\n",
    "        lr_decay = 1e-6\n",
    "        lr_drop = 20\n",
    "        # The data, shuffled and split between train and test sets:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train, x_test = self.normalize(x_train, x_test)\n",
    "\n",
    "        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n",
    "\n",
    "        def lr_scheduler(epoch):\n",
    "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "        reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "        #data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "        #optimization details\n",
    "        sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        # training process in a for loop with learning rate drop every 25 epoches.\n",
    "\n",
    "        historytemp = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                         batch_size=batch_size),\n",
    "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                            epochs=maxepoches,\n",
    "                            validation_data=(x_test, y_test),callbacks=[reduce_lr],verbose=2)\n",
    "        model.save_weights('cifar10vgg.h5')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the validation 0/1 loss is:  0.0641\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    model = cifar10vgg()\n",
    "\n",
    "    predicted_x = model.predict(x_test)\n",
    "    residuals = np.argmax(predicted_x,1)!=np.argmax(y_test,1)\n",
    "\n",
    "    loss = sum(residuals)/len(residuals)\n",
    "    print(\"the validation 0/1 loss is: \",loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cnn_to_file(X, name):\n",
    "    dims1 = X.shape[0]\n",
    "    dims2 = X.shape[1]\n",
    "    dims3 = X.shape[2]\n",
    "    dims4 = X.shape[3]\n",
    "    \n",
    "    f = open(\"./weights/\"+name+\".txt\",'w')\n",
    "    \n",
    "    for c_o in range(0,dims4):\n",
    "        for c_i in range(0,dims3):\n",
    "            for h in range(0,dims2):\n",
    "                for w in range(0,dims1):\n",
    "                    f.write(\"{0:.15}\\n\".format(X[w,h,c_i,c_o]))  \n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def write_fcn_to_file(X, name):\n",
    "    dims1 = X.shape[0]\n",
    "    dims2 = X.shape[1]\n",
    "    \n",
    "    \n",
    "    f = open(\"./weights/\"+name+\".txt\",'w')\n",
    "    \n",
    "    for c in range(0,dims2):\n",
    "        for i in range(0,dims1):\n",
    "                f.write(\"{0:.15}\\n\".format(X[i,c]))  \n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def write_image_to_file(X, name):\n",
    "    dims1 = X.shape[0]\n",
    "    dims2 = X.shape[1]\n",
    "    dims3 = X.shape[2]\n",
    "    \n",
    "    f = open(\"./data/\"+name+\".txt\",'w')\n",
    "    \n",
    "    for c in range(0,dims3):\n",
    "        for h in range(0,dims2):\n",
    "            for w in range(0,dims1):\n",
    "                f.write(\"{0:.15}\\n\".format(X[w,h,c]))  \n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 15,001,418\n",
      "Trainable params: 14,991,946\n",
      "Non-trainable params: 9,472\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "cnn = model.model\n",
    "\n",
    "#writing CNN weights\n",
    "#Writing weights and biases for CNN\n",
    "#for i in range(1,14):\n",
    "  #  write_cnn_to_file(cnn.get_layer('conv2d_'+str(i)).get_weights()[0],'w'+str(i))\n",
    "\n",
    "#for i in range(1,14):\n",
    "    #tmp = cnn.get_layer('conv2d_'+str(i)).get_weights()[1]\n",
    "   # tmp = tmp.reshape(tmp.shape[0],1)\n",
    "    #print(tmp.shape)\n",
    "   # np.savetxt('./weights/b'+str(i)+'.txt', tmp, delimiter=',')\n",
    "\n",
    "\n",
    "#Writing weights and biases for FCN\n",
    "#for i in range(1,3):\n",
    "   # write_fcn_to_file(cnn.get_layer('dense_'+str(i)).get_weights()[0],'w'+str(i+13))\n",
    "    #tmp = cnn.get_layer('dense_'+str(i)).get_weights()[1]\n",
    "    #tmp = tmp.reshape(tmp.shape[0],1)\n",
    "   # np.savetxt('./weights/b'+str(i+13)+'.txt', tmp, delimiter=',')\n",
    "\n",
    "#Writing batch norm data\n",
    "#for i in range(1,15):\n",
    "    #[gamma , beta , moving_mean, moving_variance]\n",
    "    #np.savetxt('./weights/gamma'+str(i)+'.txt', cnn.get_layer('batch_normalization_'+str(i)).get_weights()[0], delimiter=',')\n",
    "    #np.savetxt('./weights/beta'+str(i)+'.txt', cnn.get_layer('batch_normalization_'+str(i)).get_weights()[1], delimiter=',')\n",
    "    #np.savetxt('./weights/mean'+str(i)+'.txt', cnn.get_layer('batch_normalization_'+str(i)).get_weights()[2], delimiter=',')\n",
    "    #np.savetxt('./weights/variance'+str(i)+'.txt', cnn.get_layer('batch_normalization_'+str(i)).get_weights()[3], delimiter=',')\n",
    "\n",
    "\n",
    "#Helpful functions\n",
    "print(cnn.summary())\n",
    "#for i in range(0,14):\n",
    "#    print(cnn.get_layer('batch_normalization_'+str(i+1)).get_weights()[0].shape)\n",
    "#print(len(cnn.get_layer('batch_normalization_1').get_weights()))\n",
    "#print(cnn.get_layer('batch_normalization_3').get_weights()[0])\n",
    "#print(cnn.get_layer('dense_2').get_weights()[0].shape[1])\n",
    "#print(cnn.get_layer('dense_2').get_weights()[1].shape)\n",
    "#Batch Norm Data\n",
    "#[gamma , beta , moving_mean, moving_variance]\n",
    "#64     ,64    ,64          ,64  -> 64 channels!\n",
    "#w= model.model.get_layer('batch_normalization_1').get_weights()\n",
    "\n",
    "#print(x_test[0:200,:].shape)\n",
    "#x_test_normalized = model.normalize_production(x_test[0:200,:])\n",
    "#for i in range(0,200):\n",
    "    #write_image_to_file(x_test_normalized[i,:], 'image'+str(i+1))\n",
    "\n",
    "#print(np.argmax(model.predict(x_test[0:10,:]),1))\n",
    "\n",
    "\n",
    "#print(len(cnn.get_layer('conv2d_1').get_weights()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tm.add(Flatten())\\ntm.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\\ntm.add(Activation('relu'))\\ntm.add(BatchNormalization())\\ntm.add(Dropout(0.5))\\ntm.add(Dense(10))\\ntm.add(Activation('softmax'))\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weight_decay = 0.0005\n",
    "\n",
    "tm = Sequential()#test tm\n",
    "\n",
    "tm.add(Conv2D(64, (3, 3), padding='same',input_shape=[32,32,3],kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(Dropout(0.3))\n",
    "tm.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "tm.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(Dropout(0.4))\n",
    "tm.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "tm.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(Dropout(0.4))\n",
    "tm.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(Dropout(0.4))\n",
    "tm.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "tm.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(Dropout(0.4))\n",
    "tm.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(Dropout(0.4))\n",
    "tm.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "tm.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(Dropout(0.4))\n",
    "tm.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(Dropout(0.4))\n",
    "tm.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "tm.add(Dropout(0.5))\n",
    "\n",
    "\"\"\"tm.add(Flatten())\n",
    "tm.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "tm.add(Activation('relu'))\n",
    "tm.add(BatchNormalization())\n",
    "tm.add(Dropout(0.5))\n",
    "tm.add(Dense(10))\n",
    "tm.add(Activation('softmax'))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tm.get_layer('dense_3').set_weights(cnn.get_layer('dense_1').get_weights())\\ntm.get_layer('dense_4').set_weights(cnn.get_layer('dense_2').get_weights())\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.get_layer('conv2d_14').set_weights(cnn.get_layer('conv2d_1').get_weights())\n",
    "tm.get_layer('conv2d_15').set_weights(cnn.get_layer('conv2d_2').get_weights())\n",
    "tm.get_layer('conv2d_16').set_weights(cnn.get_layer('conv2d_3').get_weights())\n",
    "tm.get_layer('conv2d_17').set_weights(cnn.get_layer('conv2d_4').get_weights())\n",
    "tm.get_layer('conv2d_18').set_weights(cnn.get_layer('conv2d_5').get_weights())\n",
    "tm.get_layer('conv2d_19').set_weights(cnn.get_layer('conv2d_6').get_weights())\n",
    "tm.get_layer('conv2d_20').set_weights(cnn.get_layer('conv2d_7').get_weights())\n",
    "tm.get_layer('conv2d_21').set_weights(cnn.get_layer('conv2d_8').get_weights())\n",
    "tm.get_layer('conv2d_22').set_weights(cnn.get_layer('conv2d_9').get_weights())\n",
    "tm.get_layer('conv2d_23').set_weights(cnn.get_layer('conv2d_10').get_weights())\n",
    "tm.get_layer('conv2d_24').set_weights(cnn.get_layer('conv2d_11').get_weights())\n",
    "tm.get_layer('conv2d_25').set_weights(cnn.get_layer('conv2d_12').get_weights())\n",
    "tm.get_layer('conv2d_26').set_weights(cnn.get_layer('conv2d_13').get_weights())\n",
    "\n",
    "tm.get_layer('batch_normalization_15').set_weights(cnn.get_layer('batch_normalization_1').get_weights())\n",
    "tm.get_layer('batch_normalization_16').set_weights(cnn.get_layer('batch_normalization_2').get_weights())\n",
    "tm.get_layer('batch_normalization_17').set_weights(cnn.get_layer('batch_normalization_3').get_weights())\n",
    "tm.get_layer('batch_normalization_18').set_weights(cnn.get_layer('batch_normalization_4').get_weights())\n",
    "tm.get_layer('batch_normalization_19').set_weights(cnn.get_layer('batch_normalization_5').get_weights())\n",
    "tm.get_layer('batch_normalization_20').set_weights(cnn.get_layer('batch_normalization_6').get_weights())\n",
    "tm.get_layer('batch_normalization_21').set_weights(cnn.get_layer('batch_normalization_7').get_weights())\n",
    "tm.get_layer('batch_normalization_22').set_weights(cnn.get_layer('batch_normalization_8').get_weights())\n",
    "tm.get_layer('batch_normalization_23').set_weights(cnn.get_layer('batch_normalization_9').get_weights())\n",
    "tm.get_layer('batch_normalization_24').set_weights(cnn.get_layer('batch_normalization_10').get_weights())\n",
    "tm.get_layer('batch_normalization_25').set_weights(cnn.get_layer('batch_normalization_11').get_weights())\n",
    "tm.get_layer('batch_normalization_26').set_weights(cnn.get_layer('batch_normalization_12').get_weights())\n",
    "tm.get_layer('batch_normalization_27').set_weights(cnn.get_layer('batch_normalization_13').get_weights())\n",
    "#tm.get_layer('batch_normalization_28').set_weights(cnn.get_layer('batch_normalization_14').get_weights())\n",
    "\n",
    "\"\"\"tm.get_layer('dense_3').set_weights(cnn.get_layer('dense_1').get_weights())\n",
    "tm.get_layer('dense_4').set_weights(cnn.get_layer('dense_2').get_weights())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1869334\n",
      "-0.039582044\n",
      "0.83803517\n",
      "0.02177414\n"
     ]
    }
   ],
   "source": [
    "print(tm.get_layer('batch_normalization_15').get_weights()[0][2])\n",
    "print(tm.get_layer('batch_normalization_15').get_weights()[1][2])\n",
    "print(tm.get_layer('batch_normalization_15').get_weights()[2][2])\n",
    "print(tm.get_layer('batch_normalization_15').get_weights()[3][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08577031]\n",
      "[1.6536744]\n",
      "[-0.55397797]\n",
      "[3.4683394]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean = 120.707\n",
    "std = 64.15\n",
    "input = (x_test[0:200,:]-mean)/(std+1e-7)\n",
    "\n",
    "for i in range(0,200):\n",
    "    write_image_to_file(input[i,:], 'image'+str(i+1))\n",
    "    \n",
    "input = (x_test[0:1,:]-mean)/(std+1e-7)\n",
    "\n",
    "#print(tm.summary())\n",
    "np.set_printoptions(precision=8)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#print(m.get_layer('conv2d_20').get_weights()[0][0,2,0,0])\n",
    "print(tm.predict(input,1)[0,:,0,0])\n",
    "print(tm.predict(input,1)[0,:,0,1])\n",
    "print(tm.predict(input,1)[0,:,0,2])\n",
    "print(tm.predict(input,1)[0,:,0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
